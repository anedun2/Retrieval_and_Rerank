{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gzip\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from a CSV file and select relevant columns\n",
    "df_queries = pd.read_csv('antique_query_test.csv')\n",
    "df_queries = df_queries[['query_id','text']]\n",
    "\n",
    "df_docs = pd.read_csv('antique_sample_404k.csv')\n",
    "df_docs = df_docs[['doc_id','text']]\n",
    "\n",
    "df_qrel = pd.read_csv('antique_qurel_test.csv')\n",
    "df_qrel = df_qrel[['query_id','doc_id','relevance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the relevance data with the document data based on doc_id\n",
    "merged_df = df_qrel.merge(df_docs, on='doc_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6589 entries, 0 to 6588\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   query_id   6589 non-null   int64 \n",
      " 1   doc_id     6589 non-null   object\n",
      " 2   relevance  6589 non-null   int64 \n",
      " 3   text       6589 non-null   object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 206.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display information about the merged DataFrame\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'doc_id' and 'text' columns from the merged DataFrame\n",
    "df_text = merged_df[['doc_id','text']]\n",
    "\n",
    "# Initialize an empty list to store passages\n",
    "passages = []\n",
    "\n",
    "# Iterate through each row in the 'df_text' DataFrame and append text to the 'passages' list\n",
    "for index, row in df_text.iterrows():\n",
    "    passages.append(str(row['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Weed could mean the bad thing that grow in ur graden or back and front yard. Or it could mean the drug.',\n",
       " 'sell weed',\n",
       " 'My weed!!',\n",
       " 'because we dont know what the hell to make legal in the US anymore....i mean we still have Bush in office, you would think that legalizing weed would be less harsh of the 2',\n",
       " 'Its a weed.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 passages\n",
    "passages[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a SentenceTransformer model for embedding text\n",
    "bi_encoder = SentenceTransformer('intfloat/e5-base-v2')\n",
    "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "# top_k = 32                          #Number of passages we want to retrieve with the bi-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452babe1635d48779fb20410fa47cbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode the passages to obtain text embeddings\n",
    "corpus_embeddings_text = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the corpus embeddings to a pickle file\n",
    "import pickle\n",
    "with open('corpus_embeddings_text.pickle', 'wb') as pkl:\n",
    "    pickle.dump(corpus_embeddings_text, pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777bffbb87b041cf862c01902521c510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode the passages to obtain querey text embeddings\n",
    "query_embeddings = bi_encoder.encode(df_queries['text'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the query embeddings to a pickle file\n",
    "import pickle\n",
    "with open('query_embeddings.pickle', 'wb') as pkl:\n",
    "    pickle.dump(query_embeddings, pkl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
